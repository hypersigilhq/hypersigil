{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hypersigil Documentation","text":"<p>Welcome to Hypersigil, a comprehensive testing platform designed to help you develop, refine, and validate AI prompts before deploying them in production environments.</p>"},{"location":"#what-is-hypersigil","title":"What is Hypersigil?","text":"<p>Hypersigil transforms prompt engineering from an art into a scientific process by providing systematic testing capabilities for AI prompts. Instead of guessing whether your prompts will work effectively, you can test them against various inputs and scenarios, making decisions based on measurable results rather than intuition.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#prompt-development-testing","title":"\ud83c\udfaf Prompt Development &amp; Testing","text":"<ul> <li>Create and version your AI prompts with template variable support</li> <li>Test prompts against multiple AI providers (OpenAI, Anthropic Claude, Ollama)</li> <li>Track performance metrics and token usage across different models</li> </ul>"},{"location":"#comprehensive-test-data-management","title":"\ud83d\udcca Comprehensive Test Data Management","text":"<ul> <li>Organize test cases into logical groups</li> <li>Import test data from various file formats</li> <li>Run batch executions across entire test suites</li> </ul>"},{"location":"#execution-management","title":"\ud83d\udd04 Execution Management","text":"<ul> <li>Asynchronous processing with real-time status updates</li> <li>Provider-specific concurrency limits and rate limiting</li> <li>Detailed execution history and result analysis</li> </ul>"},{"location":"#collaborative-features","title":"\ud83d\udcac Collaborative Features","text":"<ul> <li>Comment system for execution result analysis</li> <li>Prompt calibration based on feedback</li> <li>Team collaboration with role-based access control</li> </ul>"},{"location":"#enterprise-ready","title":"\u2699\ufe0f Enterprise Ready","text":"<ul> <li>User management with invitation-based access</li> <li>API key management for external integrations</li> <li>Comprehensive audit trails and data export</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Introduction - Learn about the platform architecture and workflow</li> <li>Prompts - Understand prompt creation, versioning, and templating</li> <li>Executions - Explore the testing and execution system</li> <li>Test Data - Manage your test datasets effectively</li> <li>Settings - Configure users, API access, and system settings</li> </ol>"},{"location":"#quick-start-guide","title":"Quick Start Guide","text":""},{"location":"#first-time-setup","title":"First Time Setup","text":"<p>When you first access Hypersigil, the system will guide you through creating the initial administrator account. This establishes the foundation for your organization's prompt testing environment.</p>"},{"location":"#basic-workflow","title":"Basic Workflow","text":"<ol> <li>Create Prompts - Define your AI instructions with optional template variables</li> <li>Prepare Test Data - Import or create test cases organized in groups</li> <li>Run Executions - Test your prompts against your data using various AI providers</li> <li>Analyze Results - Review outputs, add comments, and identify improvement opportunities</li> <li>Iterate &amp; Improve - Use the calibration system to refine your prompts based on results</li> </ol>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>Hypersigil is built with a modern, scalable architecture:</p> <ul> <li>Backend: TypeScript/Node.js with Express and SQLite</li> <li>Frontend: Vue.js 3 with TypeScript and Tailwind CSS</li> <li>API: Type-safe REST API with comprehensive validation</li> <li>Providers: Extensible AI provider system supporting multiple services</li> </ul>"},{"location":"#support-contributing","title":"Support &amp; Contributing","text":"<ul> <li>Repository: GitHub</li> <li>Issues: Report bugs and request features on GitHub</li> <li>Documentation: This site is built with MkDocs and Material theme</li> </ul> <p>Ready to start testing your AI prompts systematically? Begin with the Introduction to understand the core concepts and workflow.</p>"},{"location":"executions/","title":"Executions","text":""},{"location":"executions/#executions","title":"Executions","text":"<p>Executions represent the actual testing process where your prompts are run against test data using various AI providers. When you create an execution, you're essentially asking the system to take a specific prompt, apply it to particular input data, and capture the AI's response along with metadata about the interaction.</p>"},{"location":"executions/#asynchronous-processing","title":"Asynchronous Processing","text":"<p>The execution system operates asynchronously, meaning you can queue up multiple tests and let them run in the background while you continue working on other tasks. This design becomes particularly valuable when testing prompts against large datasets or when using AI providers that may have rate limits or longer response times.</p> <p>Each execution captures comprehensive information including the exact prompt used, the input data, the AI provider and model, the response generated, timing information, and token usage. This detailed logging enables you to analyze not just what the AI produced, but also the cost and performance characteristics of different approaches.</p>"},{"location":"executions/#ai-provider-integration","title":"AI Provider Integration","text":"<p>Hypersigil integrates with multiple AI providers including OpenAI, Anthropic's Claude, and local Ollama installations. This multi-provider approach allows you to compare how different AI systems respond to the same prompts, helping you choose the most appropriate model for your specific use case.</p> <p>The provider system is designed with flexibility in mind, supporting different execution parameters for each provider. Temperature controls how creative or deterministic the AI's responses are, while token limits help manage costs and response length. The platform automatically handles the technical details of communicating with different providers while presenting a unified interface for testing.</p> <p>Token usage tracking provides visibility into the cost implications of your prompts, helping you optimize for both quality and efficiency. This information becomes crucial when scaling prompt-based applications, as small improvements in prompt efficiency can lead to significant cost savings over time.</p>"},{"location":"executions/#analyzing-execution-results","title":"Analyzing Execution Results","text":"<p>The execution results system provides multiple ways to examine and understand how your prompts perform. Individual execution details show the complete conversation flow, including the original prompt, the input data, and the AI's response. This granular view helps you understand exactly what happened during each test.</p> <p>For batch testing scenarios, the execution bundles view organizes related executions together, making it easier to analyze patterns across multiple test cases. This organizational structure becomes particularly valuable when you're testing a single prompt against dozens or hundreds of different inputs.</p> <p>The platform provides comprehensive status tracking for executions, with visual indicators showing whether tests are pending, running, completed successfully, or have failed. This real-time feedback helps you understand the progress of your testing campaigns and quickly identify any issues that need attention.</p>"},{"location":"executions/#data-export-and-system-integration","title":"Data Export and System Integration","text":"<p>The platform recognizes that prompt testing is often part of a larger development and analysis workflow. CSV export functionality allows you to extract execution data for analysis in spreadsheet applications or data analysis tools. This export capability ensures that your testing data can feed into broader reporting and decision-making processes.</p> <p>The system maintains comprehensive audit trails of all testing activity, supporting both immediate analysis and long-term trend identification. This data persistence ensures that your testing history remains available for analysis and comparison as your prompts and testing practices evolve over time.</p>"},{"location":"introduction/","title":"Introduction","text":""},{"location":"introduction/#introduction","title":"Introduction","text":"<p>Hypersigil is a comprehensive testing platform designed to help you develop, refine, and validate AI prompts before deploying them in production environments. Think of it as a laboratory where you can experiment with different ways of communicating with AI systems, measure their effectiveness, and continuously improve your results.</p> <p>The platform operates on a simple but powerful concept: instead of guessing whether your AI prompts will work well, you can systematically test them against various inputs and scenarios. This approach transforms prompt engineering from an art into a more scientific process, where decisions are based on measurable results rather than intuition.</p>"},{"location":"introduction/#getting-started-with-your-account","title":"Getting Started with Your Account","text":"<p>When you first encounter Hypersigil, the system recognizes that no users exist yet and automatically guides you through creating the first administrator account. This initial setup establishes the foundation for your organization's prompt testing environment. Once this first account is created, the system transforms into a collaborative platform where multiple team members can work together on prompt development and testing.</p> <p>The authentication system is designed around invitation-based access, meaning that after the initial administrator is established, all subsequent users join through secure invitation links. This approach ensures that access to your prompt testing environment remains controlled while making it easy to onboard new team members.</p>"},{"location":"introduction/#understanding-the-testing-workflow","title":"Understanding the Testing Workflow","text":"<p>The typical workflow in Hypersigil follows a cycle of creation, testing, analysis, and refinement. You begin by creating prompts that encode your understanding of how to communicate effectively with AI systems. These prompts are then tested against representative data to understand their performance characteristics.</p> <p>The analysis phase involves examining execution results, identifying patterns in AI responses, and understanding where prompts succeed or fail. The commenting and calibration features support this analysis by helping you capture insights and translate them into specific prompt improvements.</p> <p>This cycle repeats as you refine your prompts based on testing results, creating an iterative improvement process that leads to more effective AI interactions over time. The versioning system ensures you can track this evolution and understand how changes impact performance.</p>"},{"location":"introduction/#system-architecture-and-performance","title":"System Architecture and Performance","text":"<p>Hypersigil is designed as a comprehensive platform that handles the complexity of multi-provider AI testing while presenting a clean, intuitive interface. The system manages the technical details of communicating with different AI providers, handling rate limits, managing concurrent executions, and ensuring reliable delivery of results.</p> <p>The execution queue system ensures that your tests run efficiently regardless of provider limitations or system load. Executions are processed in the background with appropriate concurrency limits for each provider, maximizing throughput while respecting API constraints.</p> <p>This architecture enables Hypersigil to scale from individual experimentation to team-based prompt development workflows, supporting everything from quick prototype testing to comprehensive validation of production-ready prompt systems.</p>"},{"location":"prompts/","title":"Prompts","text":""},{"location":"prompts/#prompts","title":"Prompts","text":"<p>At the heart of Hypersigil lies the concept of prompts - these are the instructions or conversation starters you provide to AI systems. A prompt might be as simple as \"Summarize this text\" or as complex as a detailed persona with specific formatting requirements and behavioral guidelines. The platform treats prompts as living documents that can evolve over time through testing and refinement.</p>"},{"location":"prompts/#prompt-versioning-and-evolution","title":"Prompt Versioning and Evolution","text":"<p>Prompts in the system are versioned, meaning you can track how your instructions change over time and compare the performance of different versions. This versioning system becomes particularly valuable when you're iterating on prompt design, as you can always return to previous versions if new changes don't improve performance.</p> <p>The platform supports dynamic prompts through template variables, allowing you to create flexible instructions that can be customized for different inputs. This templating system uses Mustache syntax, enabling you to create prompts that adapt to different contexts while maintaining consistent structure and tone.</p>"},{"location":"prompts/#prompt-testing-and-calibration","title":"Prompt Testing and Calibration","text":"<p>The commenting system allows you to annotate specific parts of AI responses, creating a feedback loop that can inform prompt improvements. These comments become particularly powerful when combined with the calibration feature, which can analyze your feedback and suggest specific improvements to your prompts.</p> <p>When you have gathered feedback through comments on execution results, the calibration system can process this information and generate suggestions for how to modify your prompts to address the issues you've identified. This creates a data-driven approach to prompt improvement that goes beyond trial and error.</p>"},{"location":"settings/","title":"Settings","text":""},{"location":"settings/#settings","title":"Settings","text":"<p>The settings system in Hypersigil encompasses user management, system configuration, and integration capabilities that support both individual use and team collaboration.</p>"},{"location":"settings/#user-management-and-collaboration","title":"User Management and Collaboration","text":"<p>User management in Hypersigil is built around three distinct roles that reflect different levels of involvement in the prompt development process. Administrators have full control over the system and can manage users and settings. Regular users can create and test prompts while viewers can examine results and provide feedback without making changes.</p> <p>The invitation system ensures that bringing new team members into your prompt testing environment is both secure and straightforward. When you invite someone, they receive a secure link that allows them to set their own password and immediately begin participating in your prompt development process.</p> <p>This role-based approach enables organizations to maintain appropriate access controls while fostering collaboration between team members with different responsibilities in the prompt development process.</p>"},{"location":"settings/#api-access-and-integration","title":"API Access and Integration","text":"<p>API keys provide a way to integrate Hypersigil with other systems in your organization, enabling automated testing workflows or integration with existing development processes. These keys are scoped to specific permissions, ensuring that external integrations only have access to the functionality they need.</p> <p>The API access system enables programmatic interaction with Hypersigil, supporting scenarios where you want to automate prompt testing as part of continuous integration pipelines or integrate prompt performance data with other business intelligence systems.</p>"},{"location":"test-data/","title":"Test Data","text":""},{"location":"test-data/#test-data","title":"Test Data","text":"<p>Test data represents the various inputs and scenarios you want to evaluate your prompts against. Rather than testing prompts one input at a time, Hypersigil encourages you to build comprehensive test suites that cover the range of situations your prompts will encounter in real-world usage.</p>"},{"location":"test-data/#test-data-organization","title":"Test Data Organization","text":"<p>Test data is organized into groups, which serve as collections of related test cases. For example, you might have a group called \"Customer Support Scenarios\" containing various customer inquiries, complaints, and requests. Another group might focus on \"Technical Documentation\" with different types of technical content that need to be processed.</p> <p>This organizational structure helps you maintain logical separation between different types of testing scenarios while making it easy to run comprehensive tests across entire categories of inputs. Groups can be used to represent different use cases, different data sources, or different stages of your testing process.</p>"},{"location":"test-data/#data-import-and-management","title":"Data Import and Management","text":"<p>The platform supports both manual test data creation and bulk import from files, recognizing that test data often exists in various formats across your organization. The import system is designed to be extensible, currently supporting Markdown files with the ability to add support for other formats as needed.</p> <p>When importing data, the system intelligently processes file contents to create meaningful test cases. For Markdown files, each file becomes a separate test item with the filename serving as the item name and the file content as the test input. This approach makes it easy to convert existing documentation, examples, or datasets into structured test cases.</p>"},{"location":"test-data/#batch-testing-capabilities","title":"Batch Testing Capabilities","text":"<p>Test data groups enable powerful batch testing scenarios where you can run a single prompt against multiple test cases simultaneously. This capability transforms testing from a one-at-a-time process into a comprehensive evaluation that can cover dozens or hundreds of scenarios in a single operation.</p> <p>The batch testing system maintains the relationship between test data groups and the executions they generate, making it easy to analyze results across the entire test suite. This organizational structure supports both immediate analysis of batch results and long-term tracking of how prompt changes affect performance across different test scenarios.</p>"}]}